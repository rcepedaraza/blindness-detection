{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5b63665-1fa2-4dba-90d6-22a5cd5eb6a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T19:37:37.064362Z",
     "iopub.status.busy": "2022-12-02T19:37:37.063937Z",
     "iopub.status.idle": "2022-12-02T19:37:39.656160Z",
     "shell.execute_reply": "2022-12-02T19:37:39.654995Z",
     "shell.execute_reply.started": "2022-12-02T19:37:37.064283Z"
    }
   },
   "source": [
    "# Diabetic Retinopathy Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59ca9cd-074d-4a7b-8dd1-be8460868af4",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2092ae14-22b6-425e-bf79-594b8489f9c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T23:28:46.668354Z",
     "iopub.status.busy": "2022-12-02T23:28:46.667980Z",
     "iopub.status.idle": "2022-12-02T23:28:46.676468Z",
     "shell.execute_reply": "2022-12-02T23:28:46.675060Z",
     "shell.execute_reply.started": "2022-12-02T23:28:46.668328Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "import os\n",
    "\n",
    "from torchvision.models import resnet152, ResNet152_Weights\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils import class_weight # Estimates class weights for unbalanced datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e3bc7b-09c8-4bf9-80fe-e3340844f40f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T20:07:43.895690Z",
     "iopub.status.busy": "2022-12-02T20:07:43.895168Z",
     "iopub.status.idle": "2022-12-02T20:07:43.902827Z",
     "shell.execute_reply": "2022-12-02T20:07:43.901153Z",
     "shell.execute_reply.started": "2022-12-02T20:07:43.895650Z"
    }
   },
   "source": [
    "## Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e080a-59ec-4843-9d60-b7f54abdd220",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T23:12:34.721902Z",
     "iopub.status.busy": "2022-12-02T23:12:34.721287Z",
     "iopub.status.idle": "2022-12-02T23:12:34.728520Z",
     "shell.execute_reply": "2022-12-02T23:12:34.727279Z",
     "shell.execute_reply.started": "2022-12-02T23:12:34.721864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU device!\n"
     ]
    }
   ],
   "source": [
    "# Get device for training\n",
    "def get_device():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f'Using {device.upper()} device!')\n",
    "    return device\n",
    "\n",
    "# Addign device\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bc57d4-3c9d-47ec-8e2d-849edea5220b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T23:12:53.075627Z",
     "iopub.status.busy": "2022-12-02T23:12:53.075227Z",
     "iopub.status.idle": "2022-12-02T23:12:53.098642Z",
     "shell.execute_reply": "2022-12-02T23:12:53.097598Z",
     "shell.execute_reply.started": "2022-12-02T23:12:53.075591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available images:\t3662\n",
      "tensor([0.4058, 1.9795, 0.7331, 3.7948, 2.4827])\n"
     ]
    }
   ],
   "source": [
    "# Load images names and labels\n",
    "# path = '/notebooks/train.csv'\n",
    "# imgs_dir = '/notebooks/data/train_images'\n",
    "\n",
    "path = '../data/raw/train.csv'\n",
    "imgs_dir = '../data/raw/train_images/'\n",
    "\n",
    "# Read csv file\n",
    "df = pd.read_csv(path)\n",
    "print(f'Available images:\\t{len(df)}')\n",
    "\n",
    "# As the data is imbalanced, let's calculate the weights for each class.\n",
    "class_weights = class_weight.compute_class_weight(class_weight = 'balanced',\n",
    "                                                  classes = np.unique(df.diagnosis),\n",
    "                                                  y = df.diagnosis.values)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "print(class_weights)\n",
    "\n",
    "# Create custom class to create dataloaders\n",
    "class CustomImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Create a custom dataset for your files.\n",
    "    It must implement three functions: \n",
    "    __init__, __len__, __getitem__\n",
    "    \n",
    "    Attributes:\n",
    "    df: DataFrame with names and labels.\n",
    "    img_dir: Path to the images' folder.\n",
    "    transforms: Transforms to be applied to each image.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.img_labels = df.values\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name, label = self.img_labels[idx]\n",
    "        img_path = os.path.join(self.img_dir, f'{img_name}.png')\n",
    "        image = torchvision.io.read_image(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Transfors for train set\n",
    "transforms = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# Create train data\n",
    "dataset = CustomImageDataset(df, imgs_dir, transforms)\n",
    "\n",
    "# Set a BATCH size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Train test validation split\n",
    "train_size = 0.70\n",
    "valid_size = 0.15\n",
    "test_size = 0.15\n",
    "total_count = len(dataset)\n",
    "train_count = int(train_size * total_count)\n",
    "valid_count = int(valid_size * total_count)\n",
    "test_count = total_count - train_count - valid_count\n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, (train_count, valid_count, test_count))\n",
    "\n",
    "# Create Dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a98fff-f014-45d4-9f99-cd260e957191",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T23:12:55.445169Z",
     "iopub.status.busy": "2022-12-02T23:12:55.444814Z",
     "iopub.status.idle": "2022-12-02T23:12:56.049758Z",
     "shell.execute_reply": "2022-12-02T23:12:56.047435Z",
     "shell.execute_reply.started": "2022-12-02T23:12:55.445144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0\n",
      "Batch size:  torch.Size([32, 3, 224, 224])\n",
      "Batch label:  tensor([2, 2, 0, 2, 0, 0, 1, 4, 2, 2, 2, 1, 0, 2, 0, 0, 0, 3, 0, 4, 2, 2, 0, 4,\n",
      "        2, 0, 3, 2, 4, 0, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(train_dataloader):\n",
    "    print('Batch: ', idx),\n",
    "    print('Batch size: ', batch[0].size())\n",
    "    print('Batch label: ', batch[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883c4fc9-d1e7-4acf-afa9-a09c3cbcc4db",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca34c68-e113-4548-93b7-03967e10c8b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T23:30:16.682425Z",
     "iopub.status.busy": "2022-12-02T23:30:16.681742Z",
     "iopub.status.idle": "2022-12-02T23:30:20.067933Z",
     "shell.execute_reply": "2022-12-02T23:30:20.066675Z",
     "shell.execute_reply.started": "2022-12-02T23:30:16.682397Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transfer Learning Model\n",
    "\n",
    "# Since I do not have a lot of data, I'll use Transfer Learning\n",
    "# Downloads the resnet152 model\n",
    "model = resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "\n",
    "# Replace fully connected layer with 4 new layers\n",
    "model.fc = torch.nn.Sequential(torch.nn.Linear(2048, 256),\n",
    "                                  torch.nn.ReLU(inplace=True),\n",
    "                                  torch.nn.Linear(256, 128), \n",
    "                                  torch.nn.ReLU(inplace=True),\n",
    "                                  torch.nn.Linear(128, 64),\n",
    "                                  torch.nn.ReLU(inplace=True),\n",
    "                                  torch.nn.Linear(64, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65490e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the model\n",
    "def saveModel():\n",
    "    path = './classifier.pth'\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328c6e3-8581-4427-a489-4f4be59cf791",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T23:30:31.944515Z",
     "iopub.status.busy": "2022-12-02T23:30:31.944178Z",
     "iopub.status.idle": "2022-12-02T23:30:31.951248Z",
     "shell.execute_reply": "2022-12-02T23:30:31.950155Z",
     "shell.execute_reply.started": "2022-12-02T23:30:31.944488Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create functions for training and validation\n",
    "def train(dataloader, model, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        dataloader: Iterator for batches of images\n",
    "        model: Given an input produces an output by multiplying the input with model weights\n",
    "        criterion: loss function\n",
    "        optimizer: updates the model weights\n",
    "    Returns:\n",
    "        Average loss per batch which is calculated by dividing the losses for all\n",
    "        the batches with the number of batches\n",
    "    \"\"\"\n",
    "\n",
    "    # Sets the model for training\n",
    "    model.train()\n",
    "    device = get_device()\n",
    "\n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    total = 0\n",
    "    correct = 0 \n",
    "    running_loss = 0\n",
    "    for batch, (images, labels) in enumerate(dataloader):\n",
    "        # Move to CUDA if available\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Model predictions\n",
    "        output = model(images)\n",
    "\n",
    "        # Loss calculation\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total += labels.size(0)\n",
    "        predictions = output.argmax(dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust parametes based on the calculated gradients\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Average loss for a single batch\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "\n",
    "    # Print results\n",
    "    print(f'Training Loss:\\t{avg_loss:.6f}')\n",
    "    print(f'Accuracy on Training set:\\t{100 * (correct / total):.6f}% [{correct} / {total}]')\n",
    "    return avg_loss\n",
    "\n",
    "def validate(dataloader, model, criterion):\n",
    "    \"\"\"\n",
    "    Calculate the average loss per batch and the accuracy of the model's predictions.\n",
    "    Args: \n",
    "        dataloader: Iterator for the batches in the dataset\n",
    "        model:\n",
    "        criterion: Loss function\n",
    "    \n",
    "    Returns:\n",
    "        Average loss per batch which is calculated by dividing the losses for all the batches\n",
    "        with the number of batches\n",
    "    \"\"\"        \n",
    "\n",
    "    # Sets the model for evaluation\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    running_loss = 0.0\n",
    "    device = get_device()\n",
    "\n",
    "    # No need to calculate gradients\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            model  = model.to(device)\n",
    "\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels).item()\n",
    "            running_loss += loss\n",
    "\n",
    "            total += labels.size(0)\n",
    "            predictions = output.argmax(dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "    \n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    print(f'Validation Loss:\\t{avg_loss:.6f}')\n",
    "    print(f'Accuracy on Validation set: {100 * (correct / total):.6f}% [{correct} / {total}]')\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b43b4",
   "metadata": {},
   "source": [
    "### Optimize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1eab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(train_dataloader, valid_dataloader, model, criterion, optimizer, EPOCHS=5):\n",
    "    \"\"\"\n",
    "    Optimize function calls the train & validate functions for the number of EPOCHS\n",
    "    Args: \n",
    "        train_dataloader: \n",
    "        test_dataloader:\n",
    "        model:\n",
    "        criterion:\n",
    "        optimizer: Updates the model\n",
    "        EPOCHS: Number of epochs\n",
    "    Retuns:\n",
    "        Tuple of lists containing losses for all the epochs\n",
    "    \"\"\"\n",
    "\n",
    "    # Store losses for all the epochs\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'Epoch {epoch + 1} / {EPOCHS}')\n",
    "        print(25 * '-----')\n",
    "        train_loss = train(train_dataloader, model, criterion, optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "        valid_loss = validate(valid_dataloader, model, criterion)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        # Save the model if accuracy is the best\n",
    "        if valid_loss > best_accuracy:\n",
    "            saveModel()\n",
    "            best_accuracy = valid_loss\n",
    "\n",
    "    print(25 * '#\\nDone training!')\n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5331c5d2-3c43-44a3-bcc0-288c538d1063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "EPOCHS = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d399c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 50\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Using CPU device!\n",
      "Training Loss:\t1.613760\n",
      "Accuracy on Training set:\t7.608272% [195 / 2563]\n",
      "Using CPU device!\n",
      "Validation Loss:\t1.613496\n",
      "Accuracy on Validation set: 8.014572% [44.0 / 549.0]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'best_accuracy' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Train model step\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_losses, valid_losses \u001b[39m=\u001b[39m optimize(train_dataloader, valid_dataloader, model, criterion, optimizer, EPOCHS)\n",
      "Cell \u001b[0;32mIn [8], line 28\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(train_dataloader, valid_dataloader, model, criterion, optimizer, EPOCHS)\u001b[0m\n\u001b[1;32m     25\u001b[0m valid_losses\u001b[39m.\u001b[39mappend(valid_loss)\n\u001b[1;32m     27\u001b[0m \u001b[39m# Save the model if accuracy is the best\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[39mif\u001b[39;00m valid_loss \u001b[39m>\u001b[39m best_accuracy:\n\u001b[1;32m     29\u001b[0m     saveModel()\n\u001b[1;32m     30\u001b[0m     best_accuracy \u001b[39m=\u001b[39m valid_loss\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'best_accuracy' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Train model step\n",
    "train_losses, valid_losses = optimize(train_dataloader, valid_dataloader, model, criterion, optimizer, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b876b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "7bf60e92f4cf98d0551f32a8ec3f2a2d149318ff20c423620d69f77cd8183318"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
